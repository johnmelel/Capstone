{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee13bbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain langchain-community pymilvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab570872-e730-44c8-a799-79a5b560eee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c3f28f-902b-400c-8957-793a192de2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1050b79d-1525-4d5d-bcea-0570b011c835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb627c0",
   "metadata": {},
   "source": [
    "# Check ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d74b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there is readable file inside my folder\n",
    "#!ls \"./FINAL DATASET/\"\n",
    "\n",
    "# show whether PDF was embedded and stored in milvus\n",
    "#print(rag_system.vector_store_manager.collection.num_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b5ffc",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdae7f84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "#  Document Processing Imports\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "# Vector Store\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "# LLM and prompts\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Agent components\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# Feedback and evaluation\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Open AI Key\n",
    "'''\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "# print(os.getenv(\"OPENAI_API_KEY\")) # print API KEY \n",
    "'''\n",
    "MODEL_NAME = 'gpt-4o'\n",
    "\n",
    "# Check Milvus Connection\n",
    "def check_milvus_connection(host=\"localhost\", port=\"19530\"):\n",
    "    try:\n",
    "        from pymilvus import connections, utility\n",
    "        connections.connect(host=host, port=port)\n",
    "        collections = utility.list_collections()\n",
    "        logger.info(f\"Successfully connected to Milvus. Collections: {collections}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Milvus: {e}\")\n",
    "        logger.warning(\"Make sure Milvus is installed and running!\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1aa26e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from dotenv import load_dotenv\\nload_dotenv(dotenv_path=\"/Users/brunamedeiros/Documents/University of Chicago/Spring 2025 - Capstone I/Assignment Research 1 - Agents/.env\")\\n!ls\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/Users/brunamedeiros/Documents/University of Chicago/Spring 2025 - Capstone I/Assignment Research 1 - Agents/.env\")\n",
    "!ls\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca246966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, utility\n",
    "connections.connect(host=\"localhost\", port=\"19530\")\n",
    "print(utility.list_collections())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7de5b",
   "metadata": {},
   "source": [
    "# Set-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9eaed2",
   "metadata": {},
   "source": [
    "## Processing PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5007d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Handles document loading, chunking, and embedding\"\"\"\n",
    "    \n",
    "    # Initialize document processor\n",
    "    def __init__(self, \n",
    "                 embedding_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 chunk_size: int = 1000,\n",
    "                 chunk_overlap: int = 200):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_model_name: HuggingFace model name for embeddings\n",
    "            chunk_size: Size of text chunks\n",
    "            chunk_overlap: Overlap between chunks\n",
    "        \"\"\"\n",
    "\n",
    "        # Store constructor inputs as instance variables (so they can be used throughout the object)\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        self.embedder = HuggingFaceEmbeddings( # HuggingFaceEmbeddings from LangChain\n",
    "            model_name=embedding_model_name,\n",
    "            model_kwargs={'device': 'cuda' \n",
    "                          if os.environ.get('USE_GPU', 'false').lower() == 'true' \n",
    "                          else 'cpu'}  \n",
    "                          # automatically switches between GPU (cuda) or CPU depending on environment variable USE_GPU\n",
    "        )\n",
    "        \n",
    "        # Initialize text splitter\n",
    "        self.splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Prints model used to initialize document embedding\n",
    "        logger.info(f\"Document processor initialized with {embedding_model_name}\")\n",
    "\n",
    "    # Function: Process 1 PDF\n",
    "    def process_file(self, file_path: str) -> List[Any]: \n",
    "            \"\"\"Process a single file into chunks with metadata\"\"\"\n",
    "            logger.info(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Select appropriate loader based on file type\n",
    "            if file_path.endswith('.pdf'):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {file_path}\")\n",
    "                \n",
    "            # Load and split the document\n",
    "            documents = loader.load()\n",
    "            chunks = self.splitter.split_documents(documents)\n",
    "            \n",
    "            logger.info(f\"Generated {len(chunks)} chunks from {file_path}\")\n",
    "            return chunks\n",
    "\n",
    "    # Function: Bulk-process PDFs \n",
    "    def process_directory(self, directory_path: str) -> List[Any]:\n",
    "            \"\"\"Process all supported files in a directory\"\"\"\n",
    "            \n",
    "            # Path of folder being processed\n",
    "            logger.info(f\"Processing directory: {directory_path}\")\n",
    "            \n",
    "            # Creates folder for all PDFs files inside directory\n",
    "            loader = DirectoryLoader(  # DirectoryLoader: walks through folder recursively\n",
    "                directory_path,\n",
    "                glob=\"**/*.pdf\",       # Grabs every .pdf file\n",
    "                loader_cls=PyPDFLoader # Same as process_file\n",
    "            )\n",
    "            \n",
    "            # Loads PDFs and returns list of document objects (1 per page of each file), each with metadata\n",
    "            documents = loader.load()\n",
    "\n",
    "            # Splits documents in smaller chunks\n",
    "            chunks = self.splitter.split_documents(documents)\n",
    "            '''\n",
    "            So now you've got: \n",
    "            - chunk1 from abc.pdf, page 5\n",
    "            - chunk2 from def.pdf, page 2\n",
    "            '''\n",
    "\n",
    "            logger.info(f\"Generated {len(chunks)} chunks from {directory_path}\")\n",
    "            return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f4eb21",
   "metadata": {},
   "source": [
    "## Centralized Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d4fb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreManager:\n",
    "    \"\"\"Manages interactions with the Milvus vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 embedder: Any,\n",
    "                 host: str = \"localhost\",\n",
    "                 port: str = \"19530\", # default\n",
    "                 collection_name: str = \"knowledge_base\"):\n",
    "        \"\"\"\n",
    "        Initialize the Milvus vector store manager\n",
    "        \n",
    "        Args:\n",
    "            embedder: The embedding model to use\n",
    "            host: Milvus host address\n",
    "            port: Milvus port\n",
    "            collection_name: Name of the Milvus collection\n",
    "        \"\"\"\n",
    "\n",
    "        # Save inputs as instance variables (so other methods in the class can use them later)\n",
    "        self.embedder = embedder\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.collection_name = collection_name\n",
    "        self.connection_args = {\"host\": host, \"port\": port} # bundles host + port so it’s easier to reuse\n",
    "        self.vector_store = None                            # initialized as `none`, but will later store the actual Milvus vector store object once it's created\n",
    "        \n",
    "        logger.info(f\"Vector store manager initialized for collection: {collection_name}\")\n",
    "    \n",
    "    def initialize_from_documents(self, documents: List[Any]) -> None:\n",
    "            \"\"\"Create or update the vector store with documents\"\"\"\n",
    "            logger.info(f\"Initializing vector store with {len(documents)} documents\")\n",
    "            \n",
    "            # Check if collection exists\n",
    "            try:\n",
    "                # Try to load existing collection\n",
    "                self.vector_store = Milvus(\n",
    "                    embedding_function=self.embedder,\n",
    "                    collection_name=self.collection_name,\n",
    "                    connection_args=self.connection_args\n",
    "                )\n",
    "                logger.info(f\"Connected to existing collection: {self.collection_name}\")\n",
    "                \n",
    "                # Add new documents to existing collection\n",
    "                self.vector_store.add_documents(documents)\n",
    "                logger.info(f\"Added {len(documents)} documents to existing collection\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.info(f\"Creating new collection: {self.collection_name}\")\n",
    "                # Create new collection\n",
    "                self.vector_store = Milvus.from_documents(\n",
    "                    documents=documents,\n",
    "                    embedding=self.embedder,\n",
    "                    collection_name=self.collection_name,\n",
    "                    connection_args=self.connection_args\n",
    "                )\n",
    "                logger.info(f\"Created new collection with {len(documents)} documents\")\n",
    "        \n",
    "    def get_retriever(self, search_kwargs: Optional[Dict[str, Any]] = None) -> Any:\n",
    "        \"\"\"Get a retriever from the vector store with specified parameters\"\"\"\n",
    "        if search_kwargs is None:\n",
    "            search_kwargs = {\"k\": 5}\n",
    "            \n",
    "        if self.vector_store is None:\n",
    "            raise ValueError(\"Vector store not initialized\")\n",
    "            \n",
    "        return self.vector_store.as_retriever(search_kwargs=search_kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab333c",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ece6ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Retrieval-Augmented Generation system\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vector_store_manager: VectorStoreManager,\n",
    "                 model_name: str = \"gpt-4o\",\n",
    "                 temperature: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the RAG system\n",
    "        \n",
    "        Args:\n",
    "            vector_store_manager: Vector store manager\n",
    "            model_name: LLM model name\n",
    "            temperature: LLM temperature\n",
    "        \"\"\"\n",
    "        self.vector_store_manager = vector_store_manager\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Initialize LLM\n",
    "        self.llm = ChatOpenAI(model=model_name, temperature=temperature)\n",
    "        \n",
    "        # Default retriever parameters\n",
    "        self.retrieval_params = {\"k\": 5}\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = self.vector_store_manager.get_retriever(self.retrieval_params)\n",
    "        \n",
    "        # Define RAG prompt\n",
    "        self.prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "        You are a helpful assistant that answers questions based on the provided context.\n",
    "        \n",
    "        Context information:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer the question based on the context provided. If the context doesn't contain \n",
    "        the information needed to answer the question, admit that you don't know rather\n",
    "        than making up information.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Build the RAG chain\n",
    "        self.chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"RAG system initialized with {model_name}\")\n",
    "    \n",
    "    def answer_question(self, question: str) -> Tuple[str, List[Any]]:\n",
    "        \"\"\"\n",
    "        Answer a question using RAG\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "        \n",
    "        Returns:\n",
    "            A tuple of (answer, retrieved_documents)\n",
    "        \"\"\"\n",
    "        # Get retrieved documents (for feedback and evaluation)\n",
    "        retrieved_docs = self.retriever.invoke(question)\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.chain.invoke(question)\n",
    "        \n",
    "        return answer, retrieved_docs\n",
    "    \n",
    "    def update_retrieval_params(self, params: Dict[str, Any]) -> None:\n",
    "        \"\"\"Update retrieval parameters and refresh the retriever\"\"\"\n",
    "        self.retrieval_params.update(params)\n",
    "        self.retriever = self.vector_store_manager.get_retriever(self.retrieval_params)\n",
    "        \n",
    "        # Rebuild the chain with the new retriever\n",
    "        self.chain = (\n",
    "            {\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "            | self.prompt\n",
    "            | self.llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Updated retrieval parameters: {self.retrieval_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bae3a61",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2648bb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLFeedbackSystem:\n",
    "    \"\"\"Reinforcement Learning system for optimizing retrieval parameters\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        \"\"\"\n",
    "        Initialize the RL feedback system\n",
    "        \n",
    "        Args:\n",
    "            rag_system: The RAG system to optimize\n",
    "        \"\"\"\n",
    "        self.rag_system = rag_system\n",
    "        self.feedback_history = []\n",
    "        \n",
    "        # Parameter exploration settings\n",
    "        self.exploration_rate = 0.2  # Probability of trying new parameters\n",
    "        self.min_k = 3\n",
    "        self.max_k = 10\n",
    "        \n",
    "        # Learning rate for parameter updates\n",
    "        self.learning_rate = 0.1\n",
    "        \n",
    "        logger.info(\"RL feedback system initialized\")\n",
    "    \n",
    "    def record_feedback(self, \n",
    "                       query: str, \n",
    "                       retrieved_docs: List[Any], \n",
    "                       answer: str,\n",
    "                       feedback_score: float) -> None:\n",
    "        \"\"\"\n",
    "        Record feedback for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            retrieved_docs: The documents retrieved for the query\n",
    "            answer: The answer generated\n",
    "            feedback_score: User satisfaction score (0-1)\n",
    "        \"\"\"\n",
    "        self.feedback_history.append({\n",
    "            \"query\": query,\n",
    "            \"docs\": retrieved_docs,\n",
    "            \"answer\": answer,\n",
    "            \"score\": feedback_score,\n",
    "            \"params\": self.rag_system.retrieval_params.copy()\n",
    "        })\n",
    "        \n",
    "        logger.info(f\"Recorded feedback with score {feedback_score}\")\n",
    "        \n",
    "        # Update parameters periodically\n",
    "        if len(self.feedback_history) % 10 == 0:\n",
    "            self._update_parameters()\n",
    "    \n",
    "    def _update_parameters(self) -> None:\n",
    "        \"\"\"Update retrieval parameters based on feedback history\"\"\"\n",
    "        if len(self.feedback_history) < 10:\n",
    "            return\n",
    "            \n",
    "        recent_feedback = self.feedback_history[-10:]\n",
    "        avg_score = sum(item[\"score\"] for item in recent_feedback) / 10\n",
    "        \n",
    "        # Decide whether to explore or exploit\n",
    "        if np.random.random() < self.exploration_rate:\n",
    "            # Exploration: try a random k value\n",
    "            new_k = np.random.randint(self.min_k, self.max_k + 1)\n",
    "            logger.info(f\"Exploration: trying new k={new_k}\")\n",
    "        else:\n",
    "            # Exploitation: adjust k based on feedback\n",
    "            current_k = self.rag_system.retrieval_params.get(\"k\", 5)\n",
    "            \n",
    "            if avg_score < 0.6:\n",
    "                # If satisfaction is low, retrieve more documents\n",
    "                new_k = min(current_k + 1, self.max_k)\n",
    "                logger.info(f\"Increasing k to {new_k} due to low satisfaction\")\n",
    "            elif avg_score > 0.8:\n",
    "                # If satisfaction is high, retrieve fewer documents for efficiency\n",
    "                new_k = max(current_k - 1, self.min_k)\n",
    "                logger.info(f\"Decreasing k to {new_k} due to high satisfaction\")\n",
    "            else:\n",
    "                # Keep current k\n",
    "                new_k = current_k\n",
    "                logger.info(f\"Maintaining k at {new_k}\")\n",
    "        \n",
    "        # Update the retrieval parameters\n",
    "        self.rag_system.update_retrieval_params({\"k\": new_k})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f81a40",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e539864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSystem:\n",
    "    \"\"\"Agent system for complex query handling\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: RAGSystem):\n",
    "        \"\"\"\n",
    "        Initialize the agent system\n",
    "        \n",
    "        Args:\n",
    "            rag_system: The RAG system to use for information retrieval\n",
    "        \"\"\"\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "        # Define tools\n",
    "        self.tools = [\n",
    "            Tool(\n",
    "                name=\"KnowledgeBase\",\n",
    "                description=\"Search the knowledge base for specific information. Use this for factual questions.\",\n",
    "                func=self._search_knowledge_base\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"GenerateAnswer\",\n",
    "                description=\"Generate a comprehensive answer based on retrieved information. Use this for complex questions requiring synthesis.\",\n",
    "                func=self._generate_comprehensive_answer\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Define prompt for the agent\n",
    "        prompt_template = \"\"\"You are an intelligent agent that helps users find information and answer questions.\n",
    "        You have access to the following tools:\n",
    "        \n",
    "        {tools}\n",
    "        \n",
    "        Use the following format:\n",
    "        \n",
    "        Question: the input question you must answer\n",
    "        Thought: you should always think about what to do\n",
    "        Action: the action to take, should be one of [{tool_names}]\n",
    "        Action Input: the input to the action\n",
    "        Observation: the result of the action\n",
    "        ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "        Thought: I now know the final answer\n",
    "        Final Answer: the final answer to the original input question\n",
    "        \n",
    "        Begin!\n",
    "        \n",
    "        Question: {input}\n",
    "        Thought: \"\"\"\n",
    "        \n",
    "        # Create agent\n",
    "        self.agent = create_react_agent(\n",
    "            llm=self.rag_system.llm,\n",
    "            tools=self.tools,\n",
    "            prompt=prompt_template\n",
    "        )\n",
    "        \n",
    "        # Create agent executor\n",
    "        self.agent_executor = AgentExecutor(\n",
    "            agent=self.agent,\n",
    "            tools=self.tools,\n",
    "            verbose=True,\n",
    "            max_iterations=5\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Agent system initialized\")\n",
    "    \n",
    "    def _search_knowledge_base(self, query: str) -> str:\n",
    "        \"\"\"Tool to search the knowledge base\"\"\"\n",
    "        retrieved_docs = self.rag_system.retriever.invoke(query)\n",
    "        \n",
    "        # Format the retrieved documents as a string\n",
    "        result = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "                             for i, doc in enumerate(retrieved_docs)])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_comprehensive_answer(self, query: str) -> str:\n",
    "        \"\"\"Tool to generate a comprehensive answer\"\"\"\n",
    "        answer, _ = self.rag_system.answer_question(query)\n",
    "        return answer\n",
    "    \n",
    "    def process_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a query using the agent system\n",
    "        \n",
    "        Args:\n",
    "            query: User query\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing the agent's response and intermediate steps\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing query with agent: {query}\")\n",
    "        return self.agent_executor.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274239c",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "055ee224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveRAGSystem:\n",
    "    \"\"\"Main class that integrates all components\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                llm_model: str = \"gpt-4o\",\n",
    "                milvus_host: str = \"localhost\",\n",
    "                milvus_port: str = \"19530\",\n",
    "                collection_name: str = \"knowledge_base\"):\n",
    "        \"\"\"\n",
    "        Initialize the comprehensive RAG system\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: HuggingFace model for embeddings\n",
    "            llm_model: LLM model name\n",
    "            milvus_host: Milvus host address\n",
    "            milvus_port: Milvus port\n",
    "            collection_name: Milvus collection name\n",
    "        \"\"\"\n",
    "        # Initialize document processor\n",
    "        self.doc_processor = DocumentProcessor(embedding_model_name=embedding_model)\n",
    "        \n",
    "        # Initialize vector store manager\n",
    "        self.vector_store_manager = VectorStoreManager(\n",
    "            embedder=self.doc_processor.embedder,\n",
    "            host=milvus_host,\n",
    "            port=milvus_port,\n",
    "            collection_name=collection_name\n",
    "        )\n",
    "        \n",
    "        # Initialize RAG system\n",
    "        self.rag_system = RAGSystem(\n",
    "            vector_store_manager=self.vector_store_manager,\n",
    "            model_name=llm_model\n",
    "        )\n",
    "        \n",
    "        # Initialize RL feedback system\n",
    "        self.rl_system = RLFeedbackSystem(rag_system=self.rag_system)\n",
    "        \n",
    "        # Initialize agent system\n",
    "        self.agent_system = AgentSystem(rag_system=self.rag_system)\n",
    "        \n",
    "        logger.info(\"Comprehensive RAG system initialized\")\n",
    "    \n",
    "    def ingest_documents(self, file_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Ingest documents from a file or directory\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to file or directory\n",
    "        \"\"\"\n",
    "        if os.path.isdir(file_path):\n",
    "            chunks = self.doc_processor.process_directory(file_path)\n",
    "        else:\n",
    "            chunks = self.doc_processor.process_file(file_path)\n",
    "            \n",
    "        self.vector_store_manager.initialize_from_documents(chunks)\n",
    "        logger.info(f\"Ingested documents from {file_path}\")\n",
    "    \n",
    "    def answer_question(self, question: str, use_agent: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Answer a question using either direct RAG or the agent system\n",
    "        \n",
    "        Args:\n",
    "            question: The question to answer\n",
    "            use_agent: Whether to use the agent system\n",
    "            \n",
    "        Returns:\n",
    "            The answer to the question\n",
    "        \"\"\"\n",
    "        if use_agent:\n",
    "            result = self.agent_system.process_query(question)\n",
    "            return result[\"output\"]\n",
    "        else:\n",
    "            answer, retrieved_docs = self.rag_system.answer_question(question)\n",
    "            return answer\n",
    "    \n",
    "    def provide_feedback(self, \n",
    "                        query: str, \n",
    "                        answer: str, \n",
    "                        feedback_score: float,\n",
    "                        retrieved_docs: Optional[List[Any]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Provide feedback on a response\n",
    "        \n",
    "        Args:\n",
    "            query: The user query\n",
    "            answer: The generated answer\n",
    "            feedback_score: User satisfaction score (0-1)\n",
    "            retrieved_docs: Retrieved documents (if available)\n",
    "        \"\"\"\n",
    "        if retrieved_docs is None:\n",
    "            # Retrieve documents again if not provided\n",
    "            retrieved_docs = self.rag_system.retriever.invoke(query)\n",
    "            \n",
    "        self.rl_system.record_feedback(query, retrieved_docs, answer, feedback_score)\n",
    "        logger.info(f\"Recorded feedback with score {feedback_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87d223",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "184874a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:56:12,275 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-05-03 15:56:13,108 - INFO - Document processor initialized with sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-05-03 15:56:13,110 - INFO - Vector store manager initialized for collection: medical_knowledge_base\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Vector store not initialized",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the system\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rag_system \u001b[38;5;241m=\u001b[39m \u001b[43mComprehensiveRAGSystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers/all-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmilvus_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmilvus_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m19530\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedical_knowledge_base\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# name inside milvus\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Ingest documents\u001b[39;00m\n\u001b[1;32m     11\u001b[0m rag_system\u001b[38;5;241m.\u001b[39mingest_documents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./FINAL DATASET/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[26], line 32\u001b[0m, in \u001b[0;36mComprehensiveRAGSystem.__init__\u001b[0;34m(self, embedding_model, llm_model, milvus_host, milvus_port, collection_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store_manager \u001b[38;5;241m=\u001b[39m VectorStoreManager(\n\u001b[1;32m     25\u001b[0m     embedder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_processor\u001b[38;5;241m.\u001b[39membedder,\n\u001b[1;32m     26\u001b[0m     host\u001b[38;5;241m=\u001b[39mmilvus_host,\n\u001b[1;32m     27\u001b[0m     port\u001b[38;5;241m=\u001b[39mmilvus_port,\n\u001b[1;32m     28\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize RAG system\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrag_system \u001b[38;5;241m=\u001b[39m \u001b[43mRAGSystem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_store_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_store_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Initialize RL feedback system\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrl_system \u001b[38;5;241m=\u001b[39m RLFeedbackSystem(rag_system\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrag_system)\n",
      "Cell \u001b[0;32mIn[23], line 27\u001b[0m, in \u001b[0;36mRAGSystem.__init__\u001b[0;34m(self, vector_store_manager, model_name, temperature)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieval_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Initialize retriever\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_store_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieval_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Define RAG prompt\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124mYou are a helpful assistant that answers questions based on the provided context.\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124mthan making up information.\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 64\u001b[0m, in \u001b[0;36mVectorStoreManager.get_retriever\u001b[0;34m(self, search_kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     search_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m}\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector store not initialized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39msearch_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Vector store not initialized"
     ]
    }
   ],
   "source": [
    "# Initialize the system\n",
    "rag_system = ComprehensiveRAGSystem(\n",
    "    embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    llm_model=\"gpt-4o\",\n",
    "    milvus_host=\"localhost\",\n",
    "    milvus_port=\"19530\",\n",
    "    collection_name=\"medical_knowledge_base\" # name inside milvus\n",
    ")\n",
    "\n",
    "# Ingest documents\n",
    "rag_system.ingest_documents(\"./FINAL DATASET/\")\n",
    "\n",
    "# Now you can check the number of entities\n",
    "print(rag_system.vector_store_manager.collection.num_entities)\n",
    "\n",
    "# Answer questions\n",
    "query = \"What are the symptoms of pneumonia?\"\n",
    "\n",
    "# Simple RAG approach\n",
    "answer = rag_system.answer_question(query, use_agent=False)\n",
    "print(f\"RAG Answer:\\n{answer}\\n\")\n",
    "\n",
    "# Agent approach\n",
    "agent_answer = rag_system.answer_question(query, use_agent=True)\n",
    "print(f\"Agent Answer:\\n{agent_answer}\\n\")\n",
    "\n",
    "# Provide feedback\n",
    "rag_system.provide_feedback(query, answer, feedback_score=0.8)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
