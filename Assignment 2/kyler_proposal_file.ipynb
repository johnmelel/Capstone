{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:58.787098Z",
     "start_time": "2025-04-06T20:09:58.784566Z"
    }
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from crewai_tools import SerperDevTool"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:58.805896Z",
     "start_time": "2025-04-06T20:09:58.803595Z"
    }
   },
   "cell_type": "code",
   "source": "REVISION_ROUNDS = 1",
   "id": "8e744fc81baaf46",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Defining the Research Topic + Additional Papers",
   "id": "e0021fac457bc8fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:58.827030Z",
     "start_time": "2025-04-06T20:09:58.820111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def append_to_summary_file(text, file_path=\"Assignment 2/summaries/intermediate_steps.txt\"):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(text + \"\\n\\n\")\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def parse_serper_response(response_str):\n",
    "    \"\"\"\n",
    "    Parse the literal string output from Serper.\n",
    "    Assumes segments are separated by '---' and each segment contains lines starting with \"Title:\" and \"Snippet:\".\n",
    "    Returns a list of tuples (title, snippet).\n",
    "    \"\"\"\n",
    "    segments = response_str.split('---')\n",
    "    papers = []\n",
    "    for seg in segments:\n",
    "        seg = seg.strip()\n",
    "        if not seg:\n",
    "            continue\n",
    "        title = \"\"\n",
    "        snippet = \"\"\n",
    "        for line in seg.split('\\n'):\n",
    "            if line.startswith(\"Title:\"):\n",
    "                title = line[len(\"Title:\"):].strip()\n",
    "            elif line.startswith(\"Snippet:\"):\n",
    "                snippet = line[len(\"Snippet:\"):].strip()\n",
    "        if title or snippet:\n",
    "            papers.append((title, snippet))\n",
    "    return papers\n",
    "\n",
    "def get_additional_google_scholar_papers(query, serper):\n",
    "    # note serper output is a literal string.\n",
    "    response_str = serper.run(search_query=query)\n",
    "    parsed = parse_serper_response(response_str)\n",
    "    results_text = \"\"\n",
    "    for i, (title, snippet) in enumerate(parsed):\n",
    "        if i >= 5:\n",
    "            break\n",
    "        results_text += f\"Title: {title}\\nSnippet: {snippet}\\n\\n\"\n",
    "    if not results_text.strip():\n",
    "        results_text = \"No additional papers found from Google Scholar.\"\n",
    "    return results_text\n",
    "\n",
    "def escape_latex(text):\n",
    "    return text.replace(\"&\", r\"\\&\")\n",
    "\n",
    "def iterative_revision(text, section_name):\n",
    "    \"\"\"Iteratively revise the given LaTeX text for REVISION_ROUNDS rounds.\n",
    "    The LLM is instructed to output only the LaTeX code.\"\"\"\n",
    "    revised_text = text\n",
    "    for round in range(REVISION_ROUNDS):\n",
    "        prompt = (\n",
    "            f\"Revise the following {section_name} LaTeX code to improve clarity, structure, and formatting. \"\n",
    "            \"Output only the final LaTeX code without any additional commentary.\\n\\n\"\n",
    "            f\"{revised_text}\\n\"\n",
    "            f\"Revision Round: {round+1}\"\n",
    "        )\n",
    "        revised_text = llm.predict(prompt)\n",
    "        append_to_summary_file(f\"Revision Round {round+1} for {section_name}:\\n{revised_text}\")\n",
    "    return revised_text"
   ],
   "id": "b3419df31c51d149",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:58.848432Z",
     "start_time": "2025-04-06T20:09:58.831942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "serper = SerperDevTool(api_key=os.getenv(\"SERPER_API_KEY\"))"
   ],
   "id": "7f0802ff93a37d08",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:58.857461Z",
     "start_time": "2025-04-06T20:09:58.854769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "research_topic = (\n",
    "    \"Developing a Retrieval-Augmented Generation (RAG) LLM for retrieval of medical papers, \"\n",
    "    \"enabling a centralized vector store to mass pull papers, articles, and journals.\"\n",
    ")\n",
    "print(f\"Selected Research Topic:\\n{research_topic}\")\n",
    "append_to_summary_file(f\"Selected Research Topic:\\n{research_topic}\")"
   ],
   "id": "43c0066b16e24ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Research Topic:\n",
      "Developing a Retrieval-Augmented Generation (RAG) LLM for retrieval of medical papers, enabling a centralized vector store to mass pull papers, articles, and journals.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:09:59.921079Z",
     "start_time": "2025-04-06T20:09:58.865203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gs_query = \"RAG LLM retrieval of medical papers centralized vector store\"\n",
    "additional_papers = get_additional_google_scholar_papers(gs_query, serper)\n",
    "print(\"Additional Google Scholar Papers:\\n\", additional_papers)\n",
    "append_to_summary_file(f\"Additional Google Scholar Papers:\\n{additional_papers}\")"
   ],
   "id": "75c4d7a23acd45bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tool: Search the internet\n",
      "Additional Google Scholar Papers:\n",
      " Title: \n",
      "Snippet: RAG enhances LLM's capabilities by giving access to different information sources in real-time and seamlessly integrating them with processing.\n",
      "\n",
      "Title: Developing Retrieval Augmented Generation (RAG) based LLM ...\n",
      "Snippet: This paper presents an experience report on the development of Retrieval Augmented Generation (RAG) systems using PDF documents as the primary data source.\n",
      "\n",
      "Title: Retrieval-augmented generation for generative artificial intelligence ...\n",
      "Snippet: Retrieval-augmented generation (RAG) enables models to generate more reliable content by leveraging the retrieval of external knowledge.\n",
      "\n",
      "Title: Evaluating Medical Retrieval-Augmented Generation (RAG) with ...\n",
      "Snippet: In this overview, we'll explore RAG's growing role in healthcare, focusing on its potential to transform applications like drug discovery and clinical trials.\n",
      "\n",
      "Title: What is retrieval-augmented generation? - Red Hat\n",
      "Snippet: Retrieval-augmented generation (RAG) links external resources to an LLM to enhance a generative AI model's output accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Co-Thinking For Proposal Development",
   "id": "c5e03363641317de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:10:02.623855Z",
     "start_time": "2025-04-06T20:09:59.928914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_brainstorm = (\n",
    "    f\"Brainstorm at least 3 innovative research ideas for applying Gen AI to develop a RAG LLM for retrieval of medical papers. \"\n",
    "    \"Consider aspects like a centralized vector store, mass retrieval, and integration with academic databases. \"\n",
    "    \"Provide brief descriptions for each idea.\"\n",
    ")\n",
    "ideas = llm.predict(prompt_brainstorm)\n",
    "print(\"Brainstormed Ideas:\\n\", ideas)\n",
    "append_to_summary_file(f\"Brainstormed Ideas:\\n{ideas}\")"
   ],
   "id": "efeeb67513c0ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brainstormed Ideas:\n",
      " 1. Utilizing Gen AI to create a centralized vector store for medical papers: By leveraging Gen AI technology, researchers can develop a centralized vector store that stores embeddings of medical papers. This vector store can be used to efficiently retrieve relevant medical papers based on user queries, allowing for faster and more accurate information retrieval.\n",
      "\n",
      "2. Implementing mass retrieval capabilities using Gen AI for RAG LLM: Gen AI can be used to enhance the mass retrieval capabilities of a RAG LLM for medical papers. By training the model on a large dataset of medical papers, researchers can improve the model's ability to retrieve relevant papers in bulk, making it easier for users to access a wide range of information quickly and efficiently.\n",
      "\n",
      "3. Integrating Gen AI-powered RAG LLM with academic databases for enhanced search functionality: Researchers can integrate a Gen AI-powered RAG LLM with academic databases to provide users with enhanced search functionality. By combining the model's natural language processing capabilities with the vast amount of data available in academic databases, users can access a wealth of information on medical topics with greater ease and accuracy.\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:10:06.472012Z",
     "start_time": "2025-04-06T20:10:02.634851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_critique = (\n",
    "    f\"Critique the following research ideas for developing a RAG LLM for retrieval of medical papers. \"\n",
    "    \"Evaluate each idea on feasibility, originality, and potential impact with scores (1–5) and brief justifications.\\n\\nIdeas:\\n{ideas}\\n\"\n",
    ")\n",
    "idea_critique = llm.predict(prompt_critique)\n",
    "print(\"Idea Critiques:\\n\", idea_critique)\n",
    "append_to_summary_file(f\"Idea Critiques:\\n{idea_critique}\")"
   ],
   "id": "7d20d22b426c04d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea Critiques:\n",
      " 1. Developing a RAG LLM that incorporates domain-specific medical terminology and concepts to improve retrieval of medical papers.\n",
      "Feasibility: 4 - Feasible as there are existing resources and tools for medical terminology and concepts that can be integrated into the model.\n",
      "Originality: 3 - While incorporating domain-specific terminology is not entirely new, the focus on medical papers specifically adds a level of originality.\n",
      "Potential Impact: 5 - Improving retrieval of medical papers can have a significant impact on healthcare research and decision-making.\n",
      "\n",
      "2. Creating a RAG LLM that utilizes deep learning techniques to analyze the context and content of medical papers for more accurate retrieval.\n",
      "Feasibility: 3 - Deep learning techniques can be complex and resource-intensive, but with the right expertise and resources, it is feasible.\n",
      "Originality: 4 - Utilizing deep learning for context analysis in medical papers is a novel approach that can lead to more accurate retrieval.\n",
      "Potential Impact: 4 - More accurate retrieval of medical papers can improve the efficiency and effectiveness of healthcare research.\n",
      "\n",
      "3. Developing a RAG LLM that incorporates user feedback and preferences to personalize search results for medical papers.\n",
      "Feasibility: 5 - Personalization techniques are well-established and can be easily integrated into the model.\n",
      "Originality: 3 - While personalization is not a new concept, applying it to medical paper retrieval is a unique application.\n",
      "Potential Impact: 4 - Personalized search results can enhance user experience and lead to more relevant and useful information retrieval in the medical field.\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:10:08.931417Z",
     "start_time": "2025-04-06T20:10:06.484685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_gaps = (\n",
    "    f\"Based on academic literature, identify the key research gaps related to developing a RAG LLM for retrieval of medical papers. \"\n",
    "    \"Focus on limitations in current retrieval systems, vector store challenges, and integrating diverse academic sources.\"\n",
    ")\n",
    "research_gaps = llm.predict(prompt_gaps)\n",
    "print(\"Identified Research Gaps:\\n\", research_gaps)\n",
    "append_to_summary_file(f\"Identified Research Gaps:\\n{research_gaps}\")"
   ],
   "id": "7d75453360be5a9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Research Gaps:\n",
      " 1. Limited effectiveness of current retrieval systems: Many existing retrieval systems for medical papers rely on keyword-based search algorithms, which may not always capture the nuances of medical terminology and concepts. There is a need for more sophisticated retrieval systems that can better understand the context and semantics of medical literature.\n",
      "\n",
      "2. Challenges with vector store implementation: While vector space models have shown promise in improving retrieval accuracy, there are still challenges in implementing and optimizing these models for large-scale medical document collections. Research is needed to address issues such as scalability, efficiency, and the integration of domain-specific knowledge into vector representations.\n",
      "\n",
      "3. Integration of diverse academic sources: Medical literature is published across a wide range of academic sources, including journals, conference proceedings, and preprint repositories. Developing a RAG LLM that can effectively retrieve relevant information from these diverse sources poses a significant challenge. Future research should focus on developing methods to integrate and prioritize information from different sources based on their credibility and relevance to the user's query.\n",
      "\n",
      "Overall, addressing these research gaps will be crucial for the development of a robust and effective RAG LLM for retrieval of medical papers. By improving the accuracy, efficiency, and coverage of retrieval systems, researchers can enhance the accessibility and usability of medical literature for healthcare professionals, researchers, and other stakeholders in the medical field.\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:10:12.930664Z",
     "start_time": "2025-04-06T20:10:08.942507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_structure = (\n",
    "    f\"Generate a draft research proposal structure for the topic:\\n'{research_topic}'\\n\"\n",
    "    \"Incorporate the following:\\n\"\n",
    "    f\"Research Ideas:\\n{ideas}\\n\\n\"\n",
    "    f\"Research Gaps:\\n{research_gaps}\\n\\n\"\n",
    "    \"The structure should include the following sections:\\n\"\n",
    "    \"Title, Abstract (150–250 words), Background & Literature Review, Problem Statement & Research Gap, \"\n",
    "    \"Proposed Gen AI Approach, Expected Impact in Healthcare, Limitations or Ethical Considerations, and References.\"\n",
    ")\n",
    "proposal_structure = llm.predict(prompt_structure)\n",
    "print(\"Draft Proposal Structure:\\n\", proposal_structure)\n",
    "append_to_summary_file(f\"Draft Proposal Structure:\\n{proposal_structure}\")"
   ],
   "id": "95d4fb179b5b5baa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft Proposal Structure:\n",
      " Title: Developing a Retrieval-Augmented Generation (RAG) LLM for retrieval of medical papers using Gen AI technology\n",
      "\n",
      "Abstract:\n",
      "This research proposal aims to develop a Retrieval-Augmented Generation (RAG) LLM for the retrieval of medical papers by leveraging Gen AI technology. The proposed centralized vector store will store embeddings of medical papers to enable efficient retrieval based on user queries. By enhancing mass retrieval capabilities and integrating with academic databases, the RAG LLM aims to improve the accessibility and usability of medical literature for healthcare professionals and researchers. This research addresses the limitations of current retrieval systems and aims to bridge the gap in integrating diverse academic sources for more effective information retrieval in the medical field.\n",
      "\n",
      "Background & Literature Review:\n",
      "Existing retrieval systems for medical papers often rely on keyword-based search algorithms, which may not capture the nuances of medical terminology and concepts. There is a need for more sophisticated retrieval systems that can better understand the context and semantics of medical literature. Vector space models have shown promise in improving retrieval accuracy, but challenges remain in implementing and optimizing these models for large-scale medical document collections. Integrating diverse academic sources poses a significant challenge, requiring methods to prioritize information based on credibility and relevance.\n",
      "\n",
      "Problem Statement & Research Gap:\n",
      "The limited effectiveness of current retrieval systems and challenges with vector store implementation highlight the need for a more advanced approach to retrieving medical papers. Integrating diverse academic sources and improving the accuracy, efficiency, and coverage of retrieval systems are crucial research gaps that need to be addressed.\n",
      "\n",
      "Proposed Gen AI Approach:\n",
      "This research proposes to utilize Gen AI technology to develop a centralized vector store for medical papers, enhance mass retrieval capabilities using Gen AI for RAG LLM, and integrate the model with academic databases for enhanced search functionality. By training the model on a large dataset of medical papers and leveraging natural language processing capabilities, the RAG LLM aims to improve the retrieval of relevant medical information.\n",
      "\n",
      "Expected Impact in Healthcare:\n",
      "The proposed RAG LLM has the potential to revolutionize the retrieval of medical papers, making it easier for healthcare professionals, researchers, and other stakeholders in the medical field to access and utilize relevant information. By improving the accessibility and usability of medical literature, this research can have a significant impact on healthcare outcomes and advancements in the field.\n",
      "\n",
      "Limitations or Ethical Considerations:\n",
      "Potential limitations of this research include scalability issues with large-scale medical document collections, ethical considerations related to data privacy and security, and the need for ongoing optimization and validation of the RAG LLM. Ethical considerations will be addressed by ensuring data privacy and security measures are in place and obtaining necessary permissions for data usage.\n",
      "\n",
      "References:\n",
      "[Include relevant references related to Gen AI technology, retrieval systems, vector space models, and medical literature retrieval]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conduct Academic Literature Review",
   "id": "54e63deb266d5682"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:10:38.400499Z",
     "start_time": "2025-04-06T20:10:12.942267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "papers_dir = \"Papers\"\n",
    "paper_files = [f for f in os.listdir(papers_dir) if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "academic_summaries = \"\"\n",
    "for paper in paper_files:\n",
    "    paper_path = os.path.join(papers_dir, paper)\n",
    "    text = extract_text_from_pdf(paper_path)\n",
    "    words = text.split()\n",
    "    summary_text = \" \".join(words[:1000])\n",
    "    prompt_summary = (\n",
    "        f\"Summarize the contributions and insights of the following academic paper excerpt related to Gen AI and medical paper retrieval:\\n\\n{summary_text}\"\n",
    "    )\n",
    "    summary = llm.predict(prompt_summary)\n",
    "    academic_summaries += f\"Paper: {paper}\\nSummary:\\n{summary}\\n\\n\"\n",
    "academic_summaries += \"Additional Google Scholar Papers:\\n\" + additional_papers\n",
    "print(\"Academic Summaries:\\n\", academic_summaries)\n",
    "append_to_summary_file(f\"Academic Summaries:\\n{academic_summaries}\")"
   ],
   "id": "6a59b480fffbb3c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic Summaries:\n",
      " Paper: Systematic Review LLM Apps.pdf\n",
      "Summary:\n",
      "The academic paper excerpt highlights the current evaluation of Large Language Models (LLMs) in healthcare applications. The findings show that evaluations of LLMs in healthcare are shallow and fragmented, with a focus on accuracy and a lack of consideration for real patient care data. The paper emphasizes the need for standardized evaluations across a broad range of healthcare tasks and specialties, including the use of real patient care data and consideration of dimensions such as fairness, bias, and toxicity. The paper also discusses the potential of LLMs in improving healthcare efficiency and patient outcomes, but notes that their performance in real-world settings is inconsistently evaluated. The authors call for future studies to establish standardized evaluation metrics and broaden testing to include administrative tasks and multiple medical specialties.\n",
      "\n",
      "Paper: Transformative impact of LLM in Medicine.pdf\n",
      "Summary:\n",
      "efficient patient care. The paper highlights the transformative impact of large language models (LLMs) in health care, emphasizing their role in clinical support, diagnosis, treatment, and medical research. LLMs, such as GPT-4 and BERT, are evolving through improved computing power and data, with the ability to process multimodal data for emergency care, elder care, and digital medical procedures. Challenges include ensuring empirical reliability, addressing ethical and societal implications, mitigating biases, and maintaining privacy and accountability. The paper advocates for human-centric, bias-free LLMs for personalized medicine and equitable development and access. LLMs hold promise for transformative impacts in health care.\n",
      "\n",
      "Paper: yang-et-al-2024-application-of-large-language-models-in-disease-diagnosis-and-treatment.pdf\n",
      "Summary:\n",
      "The academic paper discusses the application of large language models (LLMs) in disease diagnosis and treatment, highlighting their ability to process vast amounts of patient data and medical literature to enhance diagnostic accuracy. The paper also introduces multimodal LLMs (MLLMs) that show promising potential for diagnosis based on various medical images. Despite the promising developments, challenges such as algorithmic bias and ethical considerations persist. The paper emphasizes the importance of policy-making, ethical supervision, and multidisciplinary collaboration in promoting more effective and safer clinical applications of LLMs. Future directions include integrating proprietary clinical knowledge, investigating open-source and customized models, and evaluating real-time effects in clinical practices.\n",
      "\n",
      "Paper: Multimodal in healthcare.pdf\n",
      "Summary:\n",
      "This academic paper discusses the use of multimodal large language models (M-LLMs) in the medical field, highlighting the importance of integrating diverse data modalities such as text, images, audio, videos, and omics data for informed clinical decisions. While large language models have shown potential in processing textual content, they often overlook the multidimensional nature of healthcare practice. The paper explores the foundational principles, applications, challenges, and future research directions of M-LLMs in healthcare, aiming to provide a comprehensive framework for their integration into medical practice. The authors emphasize the need for a paradigm shift towards integrated, multimodal data-driven medical practice and anticipate that their work will inspire innovative approaches in the next generation of medical M-LLM systems.\n",
      "\n",
      "Paper: Agents in Clinic.pdf\n",
      "Summary:\n",
      "The academic paper discusses the potential of large language models (LLMs) as intelligent agents in clinical settings, capable of interacting with stakeholders and influencing clinical decision-making. The paper emphasizes the need for evaluation frameworks, such as Artificial Intelligence Structured Clinical Examinations (AI-SCE), to assess the impact of LLM agents on clinical workflows. It highlights the capabilities of LLMs, such as generating summaries of physician-patient encounters and answering clinical questions, and suggests using agent-based modeling (ABM) to evaluate the utility and safety of LLM-based chatbots in healthcare applications. The paper also discusses the development of LLM agents for various clinical use cases and the potential for LLMs to support both routine administrative tasks and clinical decision support.\n",
      "\n",
      "Paper: Autonomous Agents 2024 in medicine.pdf\n",
      "Summary:\n",
      "The academic paper explores the use of Generative Large Language Models (LLMs) as autonomous agents in healthcare settings. The study demonstrates that LLMs can effectively function as autonomous agents by leveraging their generative capabilities and integrating with real-world data. The paper highlights the potential of LLMs to enhance decision-making in clinical settings through tailored prompts and retrieval tools. However, the study also identifies challenges such as variability in model performance and the need for ongoing manual evaluation, suggesting that further refinements in LLM technology and operational protocols are necessary to optimize their utility in healthcare.\n",
      "\n",
      "Paper: Polaris LLM Constellation.pdf\n",
      "Summary:\n",
      "The academic paper excerpt discusses the development of Polaris2, a safety-focused Large Language Model (LLM) constellation for real-time patient-AI healthcare conversations. The system is composed of multiple LLM agents working together, with a primary agent driving engaging patient-friendly conversations and specialist support agents focusing on healthcare tasks. The training protocol involves optimizing for diverse objectives and training the models on proprietary data, clinical care plans, and medical reasoning documents. The system is evaluated by over 1100 U.S. licensed nurses and 130 U.S. licensed physicians, showing performance on par with human nurses across various dimensions. Additionally, the specialist support agents outperform larger general-purpose LLMs in challenging task-based evaluations. The paper highlights the importance of safety in healthcare AI and the potential for LLMs to enhance patient care.\n",
      "\n",
      "Paper: LLM Agents in Medicine.pdf\n",
      "Summary:\n",
      "This academic paper provides a comprehensive survey of large language models (LLMs) and multimodal large language models (MLLMs) in medicine, focusing on their development, principles, application scenarios, challenges, and future directions. It highlights the paradigm shift in the medical field towards LLMs and MLLMs, reviews existing models, explores their applications in healthcare, and addresses challenges in training and deployment. The paper emphasizes the potential of LLMs and MLLMs in clinical practice and offers insights into their technical methodologies and practical clinical applications, aiming to bridge the gap between advanced technologies and clinical practice in the evolution of intelligent healthcare systems.\n",
      "\n",
      "Paper: MedAide.pdf\n",
      "Summary:\n",
      "The academic paper proposes MEDAIDE, an LLM-based omni medical multi-agent collaboration framework for specialized healthcare services. The framework aims to improve the strategic reasoning of LLMs in complex medical scenarios by decomposing multi-dimensional medical intents through query rewriting and utilizing a contextual encoder to learn intent prototype embeddings. The activated agents collaborate to provide personalized determinations with specialized medical expertise, ultimately leading to integrated decision analysis. The paper's contributions include being the first to propose an omni multi-agent collaboration framework for real-world scenarios with composite healthcare intents, improving the strategic reasoning of LLMs, and demonstrating the effectiveness of MEDAIDE through extensive experiments on medical benchmarks. The framework can be readily combined with current LLMs and provides competitive improvements.\n",
      "\n",
      "Paper: Adaptive Reasoning Language Agents.pdf\n",
      "Summary:\n",
      "The academic paper discusses the development of an adaptive large language model (LLM) agent framework for improving diagnostic accuracy in simulated clinical environments using the AgentClinic benchmark. The framework allows doctor agents to automatically correct and refine their reasoning and actions after incorrect diagnoses, leading to improved decision-making over time. The paper highlights the potential of autonomous agents in healthcare by showcasing how they can enhance diagnostic processes through adaptive learning. The simulated clinical environment of AgentClinic includes four main agents: Doctor Agent, Patient Agent, Measurement Agent, and Moderator Agent, which simulate real-time decision-making and patient interaction in clinical settings. The paper emphasizes the importance of handling cases where the doctor agent fails to provide an accurate diagnosis and introduces an automatic correction framework to address this issue. The contributions of the paper include introducing a robust adaptation mechanism for doctor agents to improve diagnostic accuracy and evaluating this framework in the AgentClinic environment to demonstrate its effectiveness in enhancing diagnostic performance through adaptive learning.\n",
      "\n",
      "Additional Google Scholar Papers:\n",
      "Title: \n",
      "Snippet: RAG enhances LLM's capabilities by giving access to different information sources in real-time and seamlessly integrating them with processing.\n",
      "\n",
      "Title: Developing Retrieval Augmented Generation (RAG) based LLM ...\n",
      "Snippet: This paper presents an experience report on the development of Retrieval Augmented Generation (RAG) systems using PDF documents as the primary data source.\n",
      "\n",
      "Title: Retrieval-augmented generation for generative artificial intelligence ...\n",
      "Snippet: Retrieval-augmented generation (RAG) enables models to generate more reliable content by leveraging the retrieval of external knowledge.\n",
      "\n",
      "Title: Evaluating Medical Retrieval-Augmented Generation (RAG) with ...\n",
      "Snippet: In this overview, we'll explore RAG's growing role in healthcare, focusing on its potential to transform applications like drug discovery and clinical trials.\n",
      "\n",
      "Title: What is retrieval-augmented generation? - Red Hat\n",
      "Snippet: Retrieval-augmented generation (RAG) links external resources to an LLM to enhance a generative AI model's output accuracy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate The Final Proposal",
   "id": "4d25acf9357797d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:14:13.245011Z",
     "start_time": "2025-04-06T20:14:13.241493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_section(section_name, context):\n",
    "    initial_prompt = (\n",
    "        f\"Write the {section_name} for a research proposal on the topic:\\n'{research_topic}'.\\nContext:\\n{context}\\n\"\n",
    "    )\n",
    "    draft = llm.predict(initial_prompt)\n",
    "    revision_prompt = (\n",
    "        f\"Revise the following {section_name} to improve clarity, structure, and academic tone:\\n{draft}\\n\"\n",
    "    )\n",
    "    revised = llm.predict(revision_prompt)\n",
    "    final_text = revised\n",
    "    append_to_summary_file(f\"{section_name} Draft:\\n{draft}\\n\\nRevised {section_name}:\\n{final_text}\")\n",
    "    return final_text"
   ],
   "id": "15c60253eca3d372",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:14:13.362171Z",
     "start_time": "2025-04-06T20:14:13.359412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title_context = \"Generate a concise, catchy title that reflects using a RAG LLM for retrieval of medical papers.\"\n",
    "abstract_context = (\n",
    "    \"Summarize the proposal in 150–250 words, including motivation, methodology (centralized vector store and RAG LLM retrieval), \"\n",
    "    \"and expected impact on academic research in healthcare.\"\n",
    ")\n",
    "bkg_context = (\n",
    "    \"Provide a comprehensive background and literature review on current methods in medical paper retrieval, \"\n",
    "    \"their limitations, and how a RAG LLM could improve the process. Include insights from the academic summaries.\"\n",
    ")\n",
    "problem_context = (\n",
    "    \"Describe the problem and research gaps identified, focusing on limitations of current retrieval systems and the challenges \"\n",
    "    \"of building a centralized vector store for academic literature.\"\n",
    ")\n",
    "approach_context = (\n",
    "    \"Detail the proposed Gen AI approach, including the architecture of the RAG LLM, integration with a vector store, \"\n",
    "    \"data processing, and experimental design.\"\n",
    ")\n",
    "impact_context = (\n",
    "    \"Discuss the expected impact on healthcare research, including improvements in literature retrieval efficiency, research speed, \"\n",
    "    \"and data accessibility.\"\n",
    ")\n",
    "limits_context = (\n",
    "    \"Identify potential limitations and ethical considerations such as data privacy, biases, and scaling challenges.\"\n",
    ")\n",
    "references_context = (\n",
    "    \"List key academic references supporting the proposal, including the papers summarized and additional relevant citations.\"\n",
    ")"
   ],
   "id": "f7cd45767db59b95",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:02.115742Z",
     "start_time": "2025-04-06T20:14:13.489376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "proposal_sections = {}\n",
    "proposal_sections[\"Title\"] = generate_section(\"Title\", title_context)\n",
    "proposal_sections[\"Abstract\"] = generate_section(\"Abstract\", abstract_context)\n",
    "proposal_sections[\"Background \\\\& Literature Review\"] = generate_section(\"Background & Literature Review\", bkg_context)\n",
    "proposal_sections[\"Problem Statement \\\\& Research Gap\"] = generate_section(\"Problem Statement & Research Gap\", problem_context)\n",
    "proposal_sections[\"Proposed Gen AI Approach\"] = generate_section(\"Proposed Gen AI Approach\", approach_context)\n",
    "proposal_sections[\"Expected Impact in Healthcare\"] = generate_section(\"Expected Impact in Healthcare\", impact_context)\n",
    "proposal_sections[\"Limitations or Ethical Considerations\"] = generate_section(\"Limitations or Ethical Considerations\", limits_context)\n",
    "proposal_sections[\"References\"] = generate_section(\"References\", references_context)"
   ],
   "id": "1bf86a34a6acee12",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate LaTeX File",
   "id": "edb367e342a09dd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:21.469716Z",
     "start_time": "2025-04-06T20:15:02.121502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "latex_prompt = (\n",
    "    \"Generate a complete LaTeX document for a research proposal on the following topic:\\n\"\n",
    "    f\"{research_topic}\\n\\n\"\n",
    "    \"The document must include the following sections with proper LaTeX formatting:\\n\"\n",
    "    \"Title, Abstract, Background & Literature Review, Problem Statement & Research Gap, \"\n",
    "    \"Proposed Gen AI Approach, Expected Impact in Healthcare, Limitations or Ethical Considerations, and References.\\n\\n\"\n",
    "    \"Use the following proposal sections:\\n\\n\"\n",
    ")\n",
    "for sec, content in proposal_sections.items():\n",
    "    latex_prompt += f\"\\\\section*{{{sec}}}\\n{content}\\n\\n\"\n",
    "\n",
    "final_latex = llm.predict(latex_prompt)\n",
    "print(\"Initial LaTeX Proposal:\\n\", final_latex)\n",
    "append_to_summary_file(\"Initial LaTeX Proposal:\\n\" + final_latex)"
   ],
   "id": "565b42f698d68bcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial LaTeX Proposal:\n",
      " \\documentclass{article}\n",
      "\\usepackage{geometry}\n",
      "\\geometry{a4paper, margin=1in}\n",
      "\\usepackage{lipsum}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\title{Utilizing RAG LLM Technology for Centralized Access to Revolutionize Medical Paper Retrieval}\n",
      "\\date{}\n",
      "\\maketitle\n",
      "\n",
      "\\section*{Abstract}\n",
      "This research proposal aims to develop a Retrieval-Augmented Generation (RAG) Language Model (LLM) specifically designed for the retrieval of medical papers. The proposed model will utilize a centralized vector store to efficiently pull papers, articles, and journals in bulk. The motivation behind this proposal is to address the challenges faced by researchers in the healthcare field when searching for relevant literature, which can often be time-consuming and ineffective.\n",
      "\n",
      "By harnessing the capabilities of the RAG LLM and a centralized vector store, researchers will be able to swiftly and accurately retrieve a large volume of medical literature. The methodology of this proposal involves training the RAG LLM on a comprehensive dataset of medical literature and integrating it with the centralized vector store to optimize retrieval efficiency.\n",
      "\n",
      "The potential impact of this research on academic research in healthcare is substantial. By streamlining the literature review process and providing researchers with access to a broader range of relevant information in a shorter timeframe, this project has the potential to drive advancements in medical research and ultimately improve healthcare outcomes.\n",
      "\n",
      "\\section*{Background \\& Literature Review}\n",
      "Background:\n",
      "\n",
      "Efficiently retrieving relevant medical literature is essential for researchers to stay informed about the latest advancements. Traditional methods like keyword searches in databases such as PubMed or Google Scholar have limitations in terms of precision, recall, and time consumption. The exponential growth of medical literature poses a challenge for researchers to keep up with the vast amount of information available. To address this challenge, advanced techniques like machine learning and natural language processing are being explored to enhance the efficiency and accuracy of information retrieval in the medical field.\n",
      "\n",
      "Literature Review:\n",
      "\n",
      "Advancements in natural language processing have led to the development of language models that can generate human-like text based on a given prompt. One notable model is the Retrieval-Augmented Generation (RAG) language model, which combines retrieval-based and generation-based capabilities to improve the quality of generated text by incorporating relevant information from a knowledge base.\n",
      "\n",
      "In the context of medical paper retrieval, utilizing a RAG LLM could transform how researchers access and extract information from a centralized vector store. By harnessing machine learning and natural language processing, a RAG LLM can not only retrieve relevant papers based on a query but also generate summaries that capture key findings and insights from the retrieved papers.\n",
      "\n",
      "Academic research in this area has highlighted the potential of RAG LLMs to streamline the literature review process in the medical field. By enabling researchers to efficiently retrieve papers from a centralized vector store, a RAG LLM can significantly reduce the time and effort required for information retrieval. This can lead to quicker dissemination of knowledge, improved decision-making, and enhanced research productivity in the medical domain.\n",
      "\n",
      "In conclusion, the development of a Retrieval-Augmented Generation (RAG) LLM for medical paper retrieval shows promise in revolutionizing how researchers access and extract information from a centralized vector store. By leveraging machine learning and natural language processing, a RAG LLM has the potential to enhance the efficiency and accuracy of information retrieval in the medical field, ultimately advancing medical research.\n",
      "\n",
      "\\section*{Problem Statement \\& Research Gap}\n",
      "Problem Statement:\n",
      "The current retrieval systems for medical papers, articles, and journals are inefficient and time-consuming, requiring users to manually search through multiple databases and sources to find relevant information. This process is labor-intensive, error-prone, and lacks a centralized vector store for academic literature, hindering researchers' ability to access and analyze information effectively.\n",
      "\n",
      "Research Gap:\n",
      "Despite advancements in information retrieval technology, there is a significant gap in the development of a comprehensive and efficient retrieval system tailored for medical literature. Existing systems struggle to accurately retrieve relevant information and are ill-equipped to handle the vast amount of data in the medical field. The absence of a centralized vector store for academic literature presents a challenge in building a cohesive retrieval system that can efficiently pull papers, articles, and journals on a large scale. This research proposal aims to address these gaps by developing a Retrieval-Augmented Generation (RAG) LLM that utilizes advanced natural language processing techniques to enhance the retrieval process and provide seamless access to medical literature through a centralized vector store.\n",
      "\n",
      "\\section*{Proposed Gen AI Approach}\n",
      "Proposed Approach for Developing a Retrieval-Augmented Generation (RAG) LLM for Medical Paper Retrieval:\n",
      "\n",
      "Our proposed approach involves the development of a Retrieval-Augmented Generation (RAG) Language Model (LLM) specifically designed for retrieving medical papers. This system will integrate advanced natural language processing techniques with a centralized vector store to efficiently pull papers, articles, and journals. The architecture of the RAG LLM will focus on retrieving relevant medical literature based on user queries and generating informative summaries or responses.\n",
      "\n",
      "Architecture of the RAG LLM:\n",
      "1. Pre-trained Language Model: The RAG LLM will be built upon a pre-trained language model, such as BERT or GPT-3, to leverage transfer learning for text understanding and generation.\n",
      "2. Retrieval Module: This module will utilize a combination of keyword matching, semantic similarity, and neural network-based retrieval techniques to identify relevant medical papers from the centralized vector store.\n",
      "3. Generation Module: The generation module will use the pre-trained language model to produce summaries or responses based on the retrieved papers, offering users concise and informative content.\n",
      "\n",
      "Integration with Vector Store:\n",
      "The centralized vector store will store embeddings of medical papers, articles, and journals, enabling efficient retrieval based on similarity metrics. The RAG LLM will interact with the vector store to retrieve relevant documents and generate responses, providing users with centralized access to a wide range of medical literature.\n",
      "\n",
      "Data Processing:\n",
      "To ensure robust performance in retrieving and generating content, the RAG LLM will be trained on a large corpus of medical literature. Data preprocessing techniques, such as tokenization, normalization, and entity recognition, will be applied to clean and structure the input data for training and inference.\n",
      "\n",
      "Experimental Design:\n",
      "1. Dataset Collection: A diverse dataset of medical papers, articles, and journals will be collected and preprocessed for training and evaluation.\n",
      "2. Model Training: The RAG LLM will be fine-tuned on the medical literature dataset using a combination of supervised and unsupervised learning techniques.\n",
      "3. Evaluation Metrics: The performance of the RAG LLM will be assessed based on metrics like retrieval accuracy, generation quality, and user satisfaction.\n",
      "4. User Study: A user study will be conducted to evaluate the usability and effectiveness of the RAG LLM in retrieving and summarizing medical literature.\n",
      "\n",
      "In conclusion, our proposed Gen AI approach aims to create an advanced system for retrieving medical papers by leveraging a centralized vector store and sophisticated language generation techniques. This system will provide users with quick and accurate access to relevant information in the medical field.\n",
      "\n",
      "\\section*{Expected Impact in Healthcare}\n",
      "The implementation of a Retrieval-Augmented Generation (RAG) LLM for the retrieval of medical papers is poised to have a profound impact on healthcare research. This cutting-edge technology will facilitate the mass retrieval of papers, articles, and journals through a centralized vector store, thereby transforming the approach to accessing and analyzing medical literature.\n",
      "\n",
      "A primary anticipated outcome of this research initiative is the enhancement of literature retrieval efficiency. Currently, researchers invest a considerable amount of time in scouring for pertinent papers and articles, a process that is both time-consuming and labor-intensive. By introducing a RAG LLM capable of swiftly retrieving medical papers, researchers will be able to access essential information more expeditiously and effortlessly, ultimately expediting the research process.\n",
      "\n",
      "Moreover, the development of this technology is expected to accelerate research speed. By enabling the mass retrieval of papers, articles, and journals from a centralized vector store, researchers will gain access to a broader spectrum of information, enabling them to conduct more comprehensive literature reviews and analyses in a shorter timeframe. This will empower researchers to generate novel insights and discoveries at an accelerated pace, thereby hastening progress in healthcare research.\n",
      "\n",
      "Furthermore, the heightened accessibility to data facilitated by this technology will yield positive implications for healthcare research. Through the centralization of medical papers, articles, and journals retrieval, researchers will have access to a more extensive array of information, including potentially overlooked or less prominent studies. This will serve to enhance the caliber and depth of research conducted within the healthcare domain, resulting in more robust and dependable findings.\n",
      "\n",
      "In conclusion, the introduction of a Retrieval-Augmented Generation (RAG) LLM for the retrieval of medical papers stands to significantly elevate healthcare research by enhancing literature retrieval efficiency, research speed, and data accessibility. This groundbreaking technology has the potential to revolutionize the manner in which researchers engage with and analyze medical literature, ultimately fostering advancements in healthcare knowledge and practice.\n",
      "\n",
      "\\section*{Limitations or Ethical Considerations}\n",
      "Limitations:\n",
      "1. Data Privacy: Concerns may arise regarding the privacy and confidentiality of medical papers stored in a centralized vector store. It is imperative to safeguard sensitive patient information to prevent any compromise.\n",
      "2. Biases: The algorithms used for retrieving medical papers may introduce biases, potentially skewing the results. Addressing and mitigating these biases during the development of the Retrieval-Augmented Generation (RAG) LLM is crucial.\n",
      "3. Scaling Challenges: The increasing volume of medical papers poses challenges in scaling the centralized vector store to efficiently handle and retrieve large amounts of data. Ensuring the system's capability to manage the growing information load is a critical consideration.\n",
      "\n",
      "Ethical Considerations:\n",
      "1. Informed Consent: Researchers must obtain informed consent from individuals whose medical papers are stored in the centralized vector store. It is essential to ensure that individuals understand how their data will be utilized and protected.\n",
      "2. Transparency: Transparency about the algorithms and processes involved in the retrieval and storage of medical papers is vital. Providing clear information about the system's operations will foster trust among users.\n",
      "3. Accountability: Researchers are accountable for the ethical implications of developing and utilizing the Retrieval-Augmented Generation (RAG) LLM. This includes ensuring responsible and ethical use of the system, as well as mitigating any potential risks.\n",
      "\n",
      "\\section*{References}\n",
      "References:\n",
      "\n",
      "1. Lewis, M., & Fan, A. (2020). \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.\" arXiv preprint arXiv:2005.11401.\n",
      "\n",
      "2. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). \"Attention is all you need.\" In Advances in Neural Information Processing Systems (pp. 5998-6008).\n",
      "\n",
      "3. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv preprint arXiv:1810.04805.\n",
      "\n",
      "4. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., & Salakhutdinov, R. (2019). \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.\" In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 2978-2988).\n",
      "\n",
      "5. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). \"Language Models are Unsupervised Multitask Learners.\" OpenAI Blog, 1(8), 9.\n",
      "\n",
      "6. Vaswani, A., & Uszkoreit, J. (2020). \"Scaling up the Transformer.\" arXiv preprint arXiv:2010.11929.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:44.032709Z",
     "start_time": "2025-04-06T20:15:21.476280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_latex_revised = iterative_revision(final_latex, \"Final LaTeX Proposal\")\n",
    "latex_file = \"Assignment 2/final_proposal.tex\"\n",
    "os.makedirs(os.path.dirname(latex_file), exist_ok=True)\n",
    "with open(latex_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_latex_revised)\n",
    "print(f\"Final LaTeX proposal written to {latex_file}\")"
   ],
   "id": "ddb4ad7eec1415b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LaTeX proposal written to Assignment 2/final_proposal.tex\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:44.254232Z",
     "start_time": "2025-04-06T20:15:44.040486Z"
    }
   },
   "cell_type": "code",
   "source": "subprocess.run([\"pdflatex\", \"-output-directory\", os.path.dirname(latex_file), latex_file])",
   "id": "a30b56e1da554da8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) (preloaded format=pdflatex)\n",
      " \\write18 enabled.\n",
      "entering extended mode\n",
      "(./Assignment 2/final_proposal.tex\n",
      "LaTeX2e <2024-11-01>\n",
      "L3 programming layer <2024-11-02>\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/base/article.cls\n",
      "Document Class: article 2024/06/29 v1.4n Standard LaTeX document class\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/base/size10.clo)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/geometry/geometry.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/keyval.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/iftex/ifvtex.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/iftex/iftex.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/lipsum/lipsum.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/l3kernel/expl3.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/l3backend/l3backend-pdftex.def))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/lipsum/lipsum.ltd.tex)) (Assignment 2/final_proposal.aux)\n",
      "*geometry* driver: auto-detecting\n",
      "*geometry* detected driver: pdftex\n",
      "\n",
      "LaTeX Warning: No \\author given.\n",
      "\n",
      "\n",
      "[1{/Users/kyler/Library/TinyTeX/texmf-var/fonts/map/pdftex/updmap/pdftex.map}]\n",
      "[2]\n",
      "! Misplaced alignment tab character &.\n",
      "l.90 1. Lewis, M., &\n",
      "                     Fan, A. (2020). \"Retrieval-Augmented Generation for Kno...\n",
      "? \n",
      "! Emergency stop.\n",
      "l.90 \n",
      "     \n",
      "!  ==> Fatal error occurred, no output PDF file produced!\n",
      "Transcript written on \"Assignment 2/final_proposal.log\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pdflatex', '-output-directory', 'Assignment 2', 'Assignment 2/final_proposal.tex'], returncode=1)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Beamer Slide Deck",
   "id": "9d6c8276189c4a31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:44.268028Z",
     "start_time": "2025-04-06T20:15:44.265640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "beamer_prompt = (\n",
    "    \"Generate a complete Beamer slide deck in LaTeX (maximum 10 slides) for the following research proposal. \"\n",
    "    \"The slides should include:\\n\"\n",
    "    \"1. A title slide (with title, author, date).\\n\"\n",
    "    \"2. An overview slide listing the proposal sections.\\n\"\n",
    "    \"3. A slide for the Abstract.\\n\"\n",
    "    \"4. A slide for Background & Literature Review.\\n\"\n",
    "    \"5. A slide for Problem Statement & Research Gap.\\n\"\n",
    "    \"6. A slide for Proposed Gen AI Approach.\\n\"\n",
    "    \"7. A slide for Expected Impact in Healthcare.\\n\"\n",
    "    \"8. A slide for Limitations or Ethical Considerations.\\n\"\n",
    "    \"9. A slide describing the system workflow and user interaction (include a description of a diagram).\\n\"\n",
    "    \"10. A conclusion/future work slide.\\n\\n\"\n",
    "    \"Use the following proposal sections in your slides:\\n\\n\"\n",
    ")\n",
    "for sec, content in proposal_sections.items():\n",
    "    beamer_prompt += f\"\\\\textbf{{{sec}}}: {content}\\n\\n\""
   ],
   "id": "5ce0317edc3e5d76",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:15:55.167739Z",
     "start_time": "2025-04-06T20:15:44.280812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "slide_deck = llm.predict(beamer_prompt)\n",
    "print(\"Initial Slide Deck LaTeX:\\n\", slide_deck)\n",
    "append_to_summary_file(\"Initial Slide Deck LaTeX:\\n\" + slide_deck)"
   ],
   "id": "ebebe6523dd156a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Slide Deck LaTeX:\n",
      " \\documentclass{beamer}\n",
      "\\usetheme{Madrid}\n",
      "\\usecolortheme{seagull}\n",
      "\n",
      "\\title{Utilizing RAG LLM Technology for Centralized Access to Revolutionize Medical Paper Retrieval}\n",
      "\\author{Author Name}\n",
      "\\date{\\today}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\begin{frame}\n",
      "\\titlepage\n",
      "\\end{frame}\n",
      "\n",
      "\\begin{frame}{Overview}\n",
      "\\tableofcontents\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Abstract}\n",
      "\\begin{frame}{Abstract}\n",
      "This research proposal aims to develop a Retrieval-Augmented Generation (RAG) Language Model (LLM) specifically designed for the retrieval of medical papers. The proposed model will utilize a centralized vector store to efficiently pull papers, articles, and journals in bulk. The motivation behind this proposal is to address the challenges faced by researchers in the healthcare field when searching for relevant literature, which can often be time-consuming and ineffective.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Background \\& Literature Review}\n",
      "\\begin{frame}{Background \\& Literature Review}\n",
      "Efficiently retrieving relevant medical literature is essential for researchers to stay informed about the latest advancements. Traditional methods like keyword searches in databases such as PubMed or Google Scholar have limitations in terms of precision, recall, and time consumption. The exponential growth of medical literature poses a challenge for researchers to keep up with the vast amount of information available. To address this challenge, advanced techniques like machine learning and natural language processing are being explored to enhance the efficiency and accuracy of information retrieval in the medical field.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Problem Statement \\& Research Gap}\n",
      "\\begin{frame}{Problem Statement \\& Research Gap}\n",
      "The current retrieval systems for medical papers, articles, and journals are inefficient and time-consuming, requiring users to manually search through multiple databases and sources to find relevant information. This process is labor-intensive, error-prone, and lacks a centralized vector store for academic literature, hindering researchers' ability to access and analyze information effectively.\n",
      "\n",
      "Research Gap:\n",
      "Despite advancements in information retrieval technology, there is a significant gap in the development of a comprehensive and efficient retrieval system tailored for medical literature. Existing systems struggle to accurately retrieve relevant information and are ill-equipped to handle the vast amount of data in the medical field. The absence of a centralized vector store for academic literature presents a challenge in building a cohesive retrieval system that can efficiently pull papers, articles, and journals on a large scale. This research proposal aims to address these gaps by developing a Retrieval-Augmented Generation (RAG) LLM that utilizes advanced natural language processing techniques to enhance the retrieval process and provide seamless access to medical literature through a centralized vector store.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Proposed Gen AI Approach}\n",
      "\\begin{frame}{Proposed Gen AI Approach}\n",
      "Our proposed approach involves the development of a Retrieval-Augmented Generation (RAG) Language Model (LLM) specifically designed for retrieving medical papers. This system will integrate advanced natural language processing techniques with a centralized vector store to efficiently pull papers, articles, and journals. The architecture of the RAG LLM will focus on retrieving relevant medical literature based on user queries and generating informative summaries or responses.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Expected Impact in Healthcare}\n",
      "\\begin{frame}{Expected Impact in Healthcare}\n",
      "The implementation of a Retrieval-Augmented Generation (RAG) LLM for the retrieval of medical papers is poised to have a profound impact on healthcare research. This cutting-edge technology will facilitate the mass retrieval of papers, articles, and journals through a centralized vector store, thereby transforming the approach to accessing and analyzing medical literature.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Limitations or Ethical Considerations}\n",
      "\\begin{frame}{Limitations or Ethical Considerations}\n",
      "Limitations:\n",
      "1. Data Privacy: Concerns may arise regarding the privacy and confidentiality of medical papers stored in a centralized vector store. It is imperative to safeguard sensitive patient information to prevent any compromise.\n",
      "2. Biases: The algorithms used for retrieving medical papers may introduce biases, potentially skewing the results. Addressing and mitigating these biases during the development of the Retrieval-Augmented Generation (RAG) LLM is crucial.\n",
      "3. Scaling Challenges: The increasing volume of medical papers poses challenges in scaling the centralized vector store to efficiently handle and retrieve large amounts of data. Ensuring the system's capability to manage the growing information load is a critical consideration.\n",
      "\n",
      "Ethical Considerations:\n",
      "1. Informed Consent: Researchers must obtain informed consent from individuals whose medical papers are stored in the centralized vector store. It is essential to ensure that individuals understand how their data will be utilized and protected.\n",
      "2. Transparency: Transparency about the algorithms and processes involved in the retrieval and storage of medical papers is vital. Providing clear information about the system's operations will foster trust among users.\n",
      "3. Accountability: Researchers are accountable for the ethical implications of developing and utilizing the Retrieval-Augmented Generation (RAG) LLM. This includes ensuring responsible and ethical use of the system, as well as mitigating any potential risks.\n",
      "\\end{frame}\n",
      "\n",
      "\\section{System Workflow and User Interaction}\n",
      "\\begin{frame}{System Workflow and User Interaction}\n",
      "\\begin{figure}\n",
      "\\includegraphics[width=0.8\\textwidth]{system_workflow.png}\n",
      "\\caption{System Workflow Diagram}\n",
      "\\end{figure}\n",
      "\\end{frame}\n",
      "\n",
      "\\section{Conclusion/Future Work}\n",
      "\\begin{frame}{Conclusion/Future Work}\n",
      "In conclusion, the development of a Retrieval-Augmented Generation (RAG) LLM for medical paper retrieval shows promise in revolutionizing how researchers access and extract information from a centralized vector store. By leveraging machine learning and natural language processing, a RAG LLM has the potential to enhance the efficiency and accuracy of information retrieval in the medical field, ultimately advancing medical research.\n",
      "\\end{frame}\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:16:08.035848Z",
     "start_time": "2025-04-06T20:15:55.182341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "slide_deck_revised = iterative_revision(slide_deck, \"Slide Deck\")\n",
    "beamer_file = \"Assignment 2/slide_deck.tex\"\n",
    "with open(beamer_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(slide_deck_revised)\n",
    "print(f\"Slide deck LaTeX written to {beamer_file}\")"
   ],
   "id": "552c64401656dcfe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide deck LaTeX written to Assignment 2/slide_deck.tex\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T20:16:11.624100Z",
     "start_time": "2025-04-06T20:16:08.039979Z"
    }
   },
   "cell_type": "code",
   "source": "subprocess.run([\"pdflatex\", \"-output-directory\", os.path.dirname(beamer_file), beamer_file])",
   "id": "c8e5a11bd8a05f0a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) (preloaded format=pdflatex)\n",
      " \\write18 enabled.\n",
      "entering extended mode\n",
      "(./Assignment 2/slide_deck.tex\n",
      "LaTeX2e <2024-11-01>\n",
      "L3 programming layer <2024-11-02>\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamer.cls\n",
      "Document Class: beamer 2024/01/06 v3.71 A class for typesetting presentations\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasemodes.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/etoolbox/etoolbox.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasedecode.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/iftex/iftex.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseoptions.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/keyval.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/geometry/geometry.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/iftex/ifvtex.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/math/pgfmath.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/pgf.revision.tex))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/utilities/pgfkeys.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/utilities/pgfkeyslibraryfiltered.code.tex))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonometric.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerarithmetics.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/base/size11.clo) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/graphicx.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/graphics.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/trig.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics-cfg/graphics.cfg) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics-def/pdftex.def))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-pdftex.def (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.def))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.tex)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/xcolor/xcolor.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics-cfg/color.cfg) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/graphics/mathcolor.ltx)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/math/pgfint.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformations.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.tex) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code.tex))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/pgf/utilities/xxcolor.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/base/atbegshi-ltx.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hyperref/hyperref.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/kvsetkeys/kvsetkeys.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/kvdefinekeys/kvdefinekeys.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pdfescape/pdfescape.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/infwarerr/infwarerr.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hycolor/hycolor.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hyperref/nameref.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/refcount/refcount.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/gettitlestring/gettitlestring.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/kvoptions/kvoptions.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/stringenc/stringenc.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hyperref/pd1enc.def) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/intcalc/intcalc.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hyperref/puenc.def) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/url/url.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/bitset/bitset.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/bigintcalc/bigintcalc.sty))\n",
      "\n",
      "Package hyperref Message: Stopped early.\n",
      "\n",
      ") (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/hyperref/hpdftex.def (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/base/atveryend-ltx.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/rerunfilecheck/rerunfilecheck.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/generic/uniquecounter/uniquecounter.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaserequires.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasecompatibility.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasefont.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsfonts/amssymb.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsfonts/amsfonts.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetranslator.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasemisc.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetwoscreens.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseoverlay.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetitle.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasesection.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseframe.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseverbatim.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseframesize.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseframecomponents.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasecolor.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasenotes.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetoc.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetemplates.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseauxtemplates.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaseboxes.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbaselocalstructure.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/tools/enumerate.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasenavigation.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasenavigationsymbols.tex)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasetheorems.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsmath/amsmath.sty\n",
      "For additional information on amsmath, use the `?' option.\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsmath/amstext.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsmath/amsgen.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsmath/amsbsy.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amsmath/amsopn.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/amscls/amsthm.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerbasethemes.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerthemedefault.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerfontthemedefault.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamercolorthemedefault.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerinnerthemedefault.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerouterthemedefault.sty))) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerthemeMadrid.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamercolorthemewhale.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamercolorthemeorchid.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerinnerthemerounded.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamerouterthemeinfolines.sty)) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/beamer/beamercolorthemeseagull.sty) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/l3backend/l3backend-pdftex.def) (Assignment 2/slide_deck.aux)\n",
      "*geometry* driver: auto-detecting\n",
      "*geometry* detected driver: pdftex\n",
      "(/Users/kyler/Library/TinyTeX/texmf-dist/tex/context/base/mkii/supp-pdf.mkii\n",
      "[Loading MPS to PDF converter (version 2006.09.02).]\n",
      ") (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/epstopdf-pkg/epstopdf-base.sty (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/latexconfig/epstopdf-sys.cfg)) (Assignment 2/slide_deck.out) (Assignment 2/slide_deck.out) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-basic-dictionary-English.dict) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-bibliography-dictionary-English.dict) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-environment-dictionary-English.dict) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-months-dictionary-English.dict) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-numbers-dictionary-English.dict) (/Users/kyler/Library/TinyTeX/texmf-dist/tex/latex/translator/translator-theorem-dictionary-English.dict) (Assignment 2/slide_deck.nav)\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 9--9\n",
      " [][][][]  \n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 13--13\n",
      " [][][][]  \n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[1{/Users/kyler/Library/TinyTeX/texmf-var/fonts/map/pdftex/updmap/pdftex.map}]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 17--17\n",
      " [][][][]  \n",
      "(Assignment 2/slide_deck.toc)\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[2]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 22--22\n",
      " [][][][]  \n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[3]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 27--27\n",
      " [][][][]  \n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[4]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 35--35\n",
      " [][][][]  \n",
      "\n",
      "Overfull \\vbox (16.85869pt too high) detected at line 35\n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[5]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 40--40\n",
      " [][][][]  \n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[6]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 45--45\n",
      " [][][][]  \n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[7]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 62--62\n",
      " [][][][]  \n",
      "\n",
      "Overfull \\vbox (127.65874pt too high) detected at line 62\n",
      "\n",
      "\n",
      "Overfull \\hbox (133.67102pt too wide) has occurred while \\output is active\n",
      " [][][][]  \n",
      "[8]\n",
      "Overfull \\hbox (133.67102pt too wide) in paragraph at lines 70--70\n",
      " [][][][]  \n",
      "\n",
      "LaTeX Warning: File `system_workflow.png' not found on input line 70.\n",
      "\n",
      "\n",
      "! Package pdftex.def Error: File `system_workflow.png' not found: using draft setting.\n",
      "\n",
      "See the pdftex.def package documentation for explanation.\n",
      "Type  H <return>  for immediate help.\n",
      " ...                                              \n",
      "                                                  \n",
      "l.70 \\end{frame}\n",
      "                \n",
      "? \n",
      "! Emergency stop.\n",
      " ...                                              \n",
      "                                                  \n",
      "l.70 \\end{frame}\n",
      "                \n",
      "!  ==> Fatal error occurred, no output PDF file produced!\n",
      "Transcript written on \"Assignment 2/slide_deck.log\".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pdflatex', '-output-directory', 'Assignment 2', 'Assignment 2/slide_deck.tex'], returncode=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9bbdfff6cd4d5b5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
